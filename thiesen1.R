## install the required packages. This needs to be done the first time you run the script
## weathercan is the package that retrieves the data from the Meteorological Service of Canada
## database https://climate.weather.gc.ca/historical_data/search_historic_data_e.html
install.packages("weathercan", repos = c("https://ropensci.r-universe.dev", "https://cloud.r-project.org"))
## sf package works with vector data
# install.packages('sf')
## leaflet package produces interactive spatial maps
# install.packages('leaflet')
## reshape2 is a package used to preprocess data frames
# install.packages('reshape2')
## ggplot2 makes nice figures
# install.packages('ggplot2')
## load the required packages
library('weathercan')
library('sf')
library('leaflet')
library('reshape2')
library('ggplot2')

# Set the working directory (location where lab_4_incomplete.R and associated data are available)
# Pay attention to forward slash /
setwd('C:/Users/14037/OneDrive - University of Calgary/Documents/ENCI_570/TM_PHES_code/TM_PHES')

# Read the shapefile of the watershed boundary using st_read()
basin <- st_read('shapefile.shp')

# Select the actual boundary (correct feature) as the rest are errors generated by the delineation algorithm
basin <- basin[2,]

# Reproject the layer to WGS 84 since weather gauges coordinates are in WGS 84.
# Use st_transform with crs=4326 as parameter. 4326 is the EPSG code of WGS 84 CRS
basin <- st_transform(basin, crs = 4326) #4326 is the EPSG code of WGS 84 CRS

# Get basin centroid coordinates to search for nearby weather stations
basin_centroid <- st_coordinates(st_centroid(basin))

# If you face an error because the geometry is not valid, you need to make it valid
# use st_make_valid()
basin <- st_make_valid(basin)

#Get basin centroid for the valid geometry
basin_centroid <- st_coordinates(st_centroid(basin))

# Search for nearby weather stations that has daily data
avail_stn <- stations_search(coords = rev(basin_centroid), # the function needs lat then lon
                             interval = 'day',  
                             dist = 25) # searches by last year of record, location, and distance

# Search for nearby weather stations that has daily data
# avail_stn <- stations_search(starts_latest = 2000,
#                             ends_earliest = 2021,
#                             coords = rev(basin_centroid), # the function needs lat then lon
#                             interval = 'day',  
#                             dist = 25) # searches by last year of record, location, and distance

print(avail_stn)
######################
# Visualize the watershed and available points
# Convert avail_stn to sf (shapefile) object to be plotted 
# Use st_as_sf() with the following parameters: coords = c('lon', 'lat'), crs= 4326
avail_stn_shp <- st_as_sf(avail_stn, coords = c('lon', 'lat'), crs= 4326)

# Create thiessen polygons from weather points
# This is a function to estimate thiessen polygon and add it to the points df
# for easier identification of each polygon

st_voronoi_point <- function(points){
  ## points must be POINT geometry
  if(!all(st_geometry_type(points) == "POINT")){
    stop("Input not  POINT geometries")
  }
  g = st_combine(st_geometry(points)) # make multipoint
  v = st_voronoi(g) #generate voronoi digram (thiessen polygons)
  v = st_collection_extract(v)
  v <- v[unlist(st_intersects(points, v))]
  v <- st_set_geometry(points, v) #put it in points df
}

# Get thiessen polygons
thiessen_poly <- st_voronoi_point(avail_stn_shp)




#plotting an interactive map

leaflet() %>% # this operator (%>%) is called pipe, which means then
  addTiles() %>% #load background image
  addCircleMarkers(data = avail_stn_shp, label = ~station_name,
                   labelOptions = labelOptions(noHide = TRUE, direction = 'top', textOnly = TRUE)) %>%
  addPolygons(data = basin, color = 'red') %>%
  addPolygons(data = thiessen_poly, label = ~station_name) 



# After this figure, we now know the gauges of interest (BANFF CS, YOHO PARK, YOHO NP OHARA LAKE), that cover the basin
# Create a dataframe with the required stations only

req_stn <- avail_stn_shp[c(1,2,3),]


#Crop thiessen polygon to the basin
# Use st_intersection(input, mask)
req_thiessen_poly <- st_intersection(thiessen_poly, basin)

# Plot the final product
leaflet() %>% # this operator is called pipe, which means then
  addTiles() %>% #load background image
  addCircleMarkers(data = req_stn, label = ~station_name,
                   labelOptions = labelOptions(noHide = TRUE, direction = 'top', textOnly = TRUE)) %>%
  addPolygons(data = basin, color = 'red') %>%
  addPolygons(data = req_thiessen_poly, label = ~station_name)


# Get a weight of each gauge to generate basin average values
req_thiessen_poly$area.m2 <- st_area(req_thiessen_poly) # Use st_area()
req_thiessen_poly$weight <- as.numeric(req_thiessen_poly$area.m2/sum(req_thiessen_poly$area.m2)) #calc weight = area_polygon/total_area


#############################

# Download the data for the required stations
weather_data <- weather_dl(station_ids = req_stn$station_id, 
      
                           interval = 'day')

#Select the date, station_name and total_precip columns only for a new precip df
precip <- weather_data[,c('date', 'station_name', 'total_precip')]

# Make the table column wise
precip <- dcast(precip, date~station_name)

#Create an empty column for basin average precip
precip$basin_average <- NA

# Get basin average values using thiessen polygons weights
# Note: the below method is very simple. Actual data need to be pre-filled with other methods.
for (i in 1:nrow(precip)) {
  #check if data from the three station are present
  pi <- as.numeric(precip[i,2:4])
  #number of NA
  if(sum(is.na(pi))==0){
    pi <- sum(pi*req_thiessen_poly$weight)
  }else if (sum(is.na(pi))==1){
    pi <- sum(pi*0.5, na.rm = T) #assume that each gauge is sharing 50% of the area
  } else if (sum(is.na(pi))==2){ #one gauge only has data
    pi <- na.omit(pi)
  }else{ # no data in all gauges
    pi <- NA
  }
  
  precip$basin_average[i] <- pi
  
}

# Bar plot precip
ggplot(data = precip)+geom_bar(aes(x = date, y=basin_average), stat = 'identity')


#do the same thing for mean temperature

temp <- weather_data[,c('date', 'station_name', 'mean_temp')]
temp <- dcast(temp, date~station_name)



#Create an empty column for basin average precip
temp$basin_average <- NA

# Get basin average values using thiessen polygons weights (calculates pi)
# Note: the below method is very simple. Actual data need to be pre-filled with other methods.
for (i in 1:nrow(temp)) {
  #check if data from the three station are present
  pi <- as.numeric(temp[i,2:4])
  #number of NA
  if(sum(is.na(pi))==0){
    pi <- sum(pi*req_thiessen_poly$weight)
  }else if (sum(is.na(pi))==1){
    pi <- sum(pi*0.5, na.rm = T) #assume that each gauge is sharing 50% of the area
  } else if (sum(is.na(pi))==2){ #one gauge only has data
    pi <- na.omit(pi)
  }else{ # no data in all gauges
    pi <- NA
  }
  
  temp$basin_average[i] <- pi
  
}

annual_temp_avg <- temp %>%
  group_by(year = as.integer(lubridate::year(date))) %>%
  summarize(basin_average = mean(basin_average, na.rm = TRUE))

ggplot(data = temp)+geom_line(aes(x = date, y=basin_average))

# If you are intersted in climate normals
#df <- stations_search(ends_earliest = 2021,
#                      coords = c(50,
#                                 -97),
#                      dist = 25,
#                      normals_years = "1981-2010") # add new parameter to specify normals

#download climate normals
#normals_dl(climate_ids = df$climate_id[1]) # all the 30-year normal data that is returned 

